<!DOCTYPE HTML>
<html>
	<head>
		<!-- Google tag (gtag.js) -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-G2VSWFJ4P2"></script>
		<script>
			window.dataLayer = window.dataLayer || [];
			function gtag(){dataLayer.push(arguments);}
			gtag('js', new Date());

			gtag('config', 'G-G2VSWFJ4P2');
		</script>
		<title>Bias and Variance</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />
		<noscript><link rel="stylesheet" href="../assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<img id="deep-torch-logo" src="../assets/images/logo.jpg" />
						<span id="deep-torch-header" class="logo">Deep Torch</span>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
						</ul>
						<ul class="icons">
							<li><a href="https://www.linkedin.com/company/93786086/" class="icon brands fa-linkedin"><span class="label">Linked in</span></a></li>
							<li><a href="https://www.facebook.com/profile.php?id=100091574053236&mibextid=LQQJ4d" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									<span class="date">May 20, 2023</span>
									<p style="font-size: 4rem; font-style: normal; font-weight: bolder;">
                                        Bias and Variance
                                    </p>
								</header>
								<div >
									<p>
										When we train a model, maybe we’ll face some challenges like Overfitting and Underfitting, What Does That Mean? 
										<br>
										Well, we'll discuss that clearly.
									</p>
									<p>
										Now, Overfitting is a scenario that occurs when a statistical model fits exactly against the training data. It learns the training data so well that it almost fits it. Since it leads to high variance and little bias, it performs poorly on the test dataset. Which means that the model does not generalize well even on the test data. 
									</p>
									<p>
										On the other hand, Underfitting is a scenario when the model performs poorly on both train and test datasets. It is a scenario of low variance and high bias. 
									</p>
									<div>
										<p>
											Before diving in further let’s understand two important terms: 
										</p>
										<ul>
											<li>
												<b>Bias:</b> refers to the difference between the expected output of a model and the true output. In other words, it measures how well a model fits the training data. A model with high bias is typically too simple, and may not be able to capture the complexity of the underlying data. This can lead to underfitting, where the model performs poorly on both the training data and new, unseen data. 
											</li>
											<li>
												<b>Variance:</b> on the other hand, measures the sensitivity of a model to small fluctuations in the training data. Which means if we change the data a little by adding some noise for example, the model will change dramatically, even though the data still have the same distribution. A model with high variance is typically too complex, and may be overfitting the training data. This means that the model is able to fit the training data very well, but does not generalize well to new, unseen data.  
											</li>
										</ul>
									</div>

									<div class="image main"><img src="../assets/images/bias-variance-parameter-space.png" alt="bias and variance in parameters space" /></div>

									<p>
										It's important to recognize that the bias and variance aren't Overfitting and underfitting, if we don't have a perfect bias and variance, we will have Overfitting OR Underfitting.
										<br>
										Maybe we will face high bias and high variance but we will never have Underfit and Overfit at the same time. 
									</p>
									<div class="image main"><img src="../assets/images/underfitting-overfitting.jpg" alt="underfitting and overfitting" /></div>
									<div>
										<p>Empirically we can detect bias as follows: </p>
										<ul>
											<li> large training error => large bias. </li>
											<li>small training error => small bias. </li>
										</ul>
									</div>
									<p>
										This means that the bias is a way of describing the difference between the actual true relationship in our data and the one our model learned. 
									</p>
									<div>
										<p>Empirically we can detect variance as follows: </p>
										<ul>
											<li>
												If a model has high variance. In this case, the model may perform very well on the training set, but poorly on the validation set, indicating that it is not a good fit for the data. 
											</li>
											<li>
												if a model has low variance. In this case, the model may perform well on both the training set and the validation set, indicating that it is a good fit for the data. 
											</li>
										</ul>
									</div>
									<p>
										<b>
											if a model has low variance. In this case, the model may perform well on both the training set and the validation set, indicating that it is a good fit for the data. 
										</b>
									</p>
									<p>
										Let’s think about how we might decrease bias. The easiest way to achieve a lower bias would be to pick a more complex model or to train our existing model for longer. In other words, we want to extract more insights from our dataset, we want our model to learn more from our data. The more our model learns from our data, the better it will be at solving its task. 
									</p>
									<p>
										Meanwhile, the term "variance" refers to the amount of variability or flexibility of a model in fitting the training data. A model with high variance can fit the data very closely, including the noise, while a model with low variance is less flexible and cannot fit the training data as closely, even if there is some noise in the data. 
									</p>
									<p>
										In machine learning, the term "noise" generally refers to the random variability or errors in the data that are not related to the underlying patterns or relationships that the model is trying to learn. Noise can arise due to various factors such as measurement errors, data collection issues, or other sources of variability in the data. 
									</p>
									<p>
										In other words, noise is the unwanted random variability in the data, while variance is the ability of the model to capture or fit that variability. A model with high variance is more likely to fit the noise in the data, while a model with low variance is more likely to ignore the noise and capture the underlying patterns. 
									</p>
									<p>
										So there seems to be a sort of “tradeoff” between bias and variance. You can only decrease your bias so much until your variance starts to increase. At this point, you might think of a graph like this: 
									</p>
									<div class="image main"><img src="../assets/images/bias-variance-tradeoff.jpg" alt="bias and variance tradeoff" /></div>

									<div>
										<p>Here are some common reasons why overfitting can occur: </p>
										<ul>
											<li>
												<b>Insufficient training data:</b> If the amount of training data is too small, the model may memorize the training examples instead of learning the underlying patterns, resulting in overfitting. 
											</li>
											<li>
												<b>Overly complex model:</b> If the model is too complex, it may have too many parameters or be too flexible, allowing it to fit the noise in the training data instead of the underlying patterns. 
											</li>
											<li>
												<b>Outliers:</b> If the training data contains outliers, extreme values that are far from the typical range of values, the model may overfit to these outliers and perform poorly on typical data. 
											</li>
										</ul>
									</div>
									<div>
										<p>On the other hand, underfitting can occur when: </p>
										<ul>
											<li>
												<b>Insufficient model complexity:</b> If the model is too simple or has too few parameters, it may not be able to capture the complexity of the underlying patterns in the training data, resulting in underfitting. 
											</li>
											<li>
												<b>Insufficient training time:</b> If the model is not trained for enough epochs or iterations, it may not have enough time to learn the underlying patterns in the training data, resulting in underfitting. 
											</li>
											<li>
												<b>Feature selection:</b> If the features or variables used to train the model are not relevant to the problem or do not capture the underlying patterns, the model may underfit. 
											</li>
										</ul>
									</div>
									<p><b>Now, how do you solve these problems? </b></p>
									<div>
										<p>Let's start with the most complex one: overfitting, there are four main techniques:</p>
										<h2>Regularization</h2>
										<p>
											Regularization is one of the most important concepts of machine learning. It is a technique used to prevent the model from overfitting. this technique can be used in such a way that it allows maintaining all variables or features in the model by reducing the magnitude of the variables, hence, aside from maintains accuracy, it also maintains a generalization of the model. 
										</p>
										<p>
											In regularization technique we can reduce the magnitude of the features while attaining the same number of the features. 
											<br>
											To understand Regularization more, we will talk about in depth in an upcoming article.  
										</p>
										<div class="image main"><img src="../assets/images/regularization.jpg" alt="regularization" /></div>

										<h2>Data augmentation</h2>
										<p>
											This is a set of techniques used to artificially increase the size of a dataset by applying transformations to the existing data. For instance, in the case of images, you can flip images horizontally or vertically, crop them or rotate them. You can also turn them into grayscale or change the color saturation.
											<br>
											As far as the algorithm is concerned, new data has been created. Of course, not all transformations are useful in every case. And in some cases, your algorithm won’t be fooled.
											<br>
											In short, data augmentation can be a very powerful tool but it requires a careful examination and understanding of your data. 
										</p>
										<h2>Removing features from data</h2>
										<p>
											Sometimes, your model may fail to generalize simply because the data it was trained on was too complex and the model missed the patterns it should have detected. Removing some features and making your data simpler can help reduce overfitting. 
										</p>
										<h2>Adding more data</h2>
										<p>
											Your model is overfitting when it fails to generalize to new data. That means the data it was trained on is not representative of the data it is meeting in production. So, retraining your algorithm on a bigger, richer and more diverse data set should improve its performance.
											<br>
											Unfortunately, getting more data can prove to be very difficult; either because collecting it is very expensive or because very few samples are regularly generated. In that case, it might be a good idea to use data augmentation.
										</p>
									</div>
									<p>
										It is important to understand that overfitting is a complex problem. You will almost systematically face it when you develop a deep learning model and you should not feel discouraged if you are struggling to address it. Even the most experienced ML engineers spend a lot of time trying to solve it. 
									</p>
									<div>
										<p>
											Now the problem of underfitting: Here is what we can try: 
										</p>
										<h2>Increasing the model complexity</h2>
										<p>
											Your model may be underfitting simply because it is not complex enough to capture patterns in the data. Using a more complex model, for instance by switching from a linear to a non-linear model or by adding hidden layers to your neural network, will very often help solve underfitting. 
										</p>
										<h2>Reducing regularization</h2>
										<p>
											Default regularization parameters are often included in algorithms  to prevent overfitting, but sometimes they can hinder learning.   Decreasing their values usually alleviates this issue. 
										</p>
										<h2>Adding features to training data</h2>
										<p>
											In contrast to overfitting, your model may be underfitting because the training data is too simple. It may lack the features that will make the model detect the relevant patterns to make accurate predictions. Adding features and complexity to your data can help overcome underfitting. 
										</p>
									</div>
									<p>
										Adding more data is not included in the techniques to solve underfitting. Indeed, if your data is lacking the decisive features to allow your model to detect patterns, you can multiply your training set size by 2, 5 or even 10, it won’t make your algorithm better! 
									</p>
									<p>
										Unfortunately, it has become a reflex in the industry. No matter what the problem their model is facing, a lot of engineers think that throwing more data at it will solve the problem. When you know how time-consuming and expensive it can be to collect data, this is a mistake that can seriously harm or even jeopardize a project. 
									</p>
								</div>
							</section>

					</div>

				<!-- Footer -->

			</div>

		<!-- Scripts -->
			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/jquery.scrollex.min.js"></script>
			<script src="../assets/js/jquery.scrolly.min.js"></script>
			<script src="..assets/js/browser.min.js"></script>
			<script src="../assets/js/breakpoints.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<script src="../assets/js/main.js"></script>

	</body>
</html>